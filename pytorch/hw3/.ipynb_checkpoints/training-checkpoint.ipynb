{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation\n",
    "from torchnlp.nn import LockedDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "dataset = np.load('../dataset/wiki.train.npy')\n",
    "dataser_val = np.load('../dataset/wiki.valid.npy')\n",
    "fixtures_pred = np.load('../fixtures/dev_fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/dev_fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/test_fixtures/prediction.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/test_fixtures/generation.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text, seq_len = 70):\n",
    "        text = combine(text)\n",
    "        n_seq = len(text) // seq_len\n",
    "        text = text[:n_seq * seq_len]\n",
    "        self.data = torch.tensor(text).view(-1,seq_len)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        txt = self.data[i]\n",
    "        return txt[:-1],txt[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "def collate(seq_list):\n",
    "    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    return inputs,targets\n",
    "\n",
    "def combine(text):\n",
    "    result = np.array([])\n",
    "    for i in dataset:\n",
    "        result = np.concatenate((result, i))\n",
    "    return result.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, charcount):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.vocab_size = charcount\n",
    "        self.embed_size = 400 # From Paper\n",
    "        self.hidden_size = 1150 # From Paper\n",
    "        self.nlayers = 3 # From Paper\n",
    "        self.embed_dropout = 0.1\n",
    "        self.output_dropout = 0.4\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.rnn = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.nlayers)\n",
    "        self.scoring = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "        # Regularizations\n",
    "        # Embedding Dropout\n",
    "#         self.embedding_dropout = LockedDropout(self.embed_dropout)\n",
    "#         self.outputting_dropout = LockedDropout(self.output_dropout)\n",
    "\n",
    "#         # Weight tying\n",
    "#         self.scoring.weight = self.embedding.weight\n",
    "\n",
    "#         #weight inilization \n",
    "#         self.init_weights()\n",
    "\n",
    "    def forward(self, seq_batch):\n",
    "        batch_size = seq_batch.size(1)\n",
    "#         embed = self.embedding_dropout(self.embedding(seq_batch))\n",
    "        embed = self.embedding(seq_batch)\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed, hidden)\n",
    "#         output = self.outputting_dropout(output_lstm)\n",
    "        output = output_lstm\n",
    "#         output_lstm_flatten = output.view(-1,self.hidden_size)\n",
    "#         output_flatten = self.scoring(output_lstm_flatten)\n",
    "        output_flatten = self.scoring(output)\n",
    "        return output_flatten.view(-1, batch_size, self.vocab_size), hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        val = 0.1\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        self.scoring.bias.data.fill_(0)\n",
    "        self.scoring.weight.data.uniform_(-val, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "#         self.train_losses = []\n",
    "#         self.val_losses = []\n",
    "#         self.predictions = []\n",
    "#         self.predictions_test = []\n",
    "#         self.generated_logits = []\n",
    "#         self.generated = []\n",
    "#         self.generated_logits_test = []\n",
    "#         self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'%(self.epochs, self.max_epochs, epoch_loss))\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        output, hidden = self.model(inputs)\n",
    "        loss = self.criterion(output.view(-1, self.model.vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "#     def test(self):\n",
    "#         # don't change these\n",
    "#         self.model.eval() # set to eval mode\n",
    "#         predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "#         self.predictions.append(predictions)\n",
    "#         nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        \n",
    "#         generated_logits = TestLanguageModel.generation(fixtures_gen, 20, self.model) # predictions for 20 words\n",
    "#         generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 20, self.model) # predictions for 20 words\n",
    "\n",
    "#         generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "#         generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "#         self.val_losses.append(nll)\n",
    "        \n",
    "#         self.generated.append(generated)\n",
    "#         self.generated_test.append(generated_test)\n",
    "#         self.generated_logits.append(generated_logits)\n",
    "#         self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "#         # generate predictions for test data\n",
    "#         predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "#         self.predictions_test.append(predictions_test)\n",
    "            \n",
    "#         print('[VAL]  Epoch [%d/%d]   NLL: %.4f'\n",
    "#                       % (self.epochs, self.max_epochs, nll))\n",
    "#         return nll\n",
    "\n",
    "#     def save(self):\n",
    "#         # don't change these\n",
    "#         model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "#         torch.save({'state_dict': self.model.state_dict()},\n",
    "#             model_path)\n",
    "#         np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "#         np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "#         np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "#         np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "#         with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "#             fw.write(self.generated[-1])\n",
    "#         with open(os.path.join('experiments', self.run_id, 'generated-test-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "#             fw.write(self.generated_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = str(int(time.time()))\n",
    "# if not os.path.exists('./experiments'):\n",
    "#     os.mkdir('./experiments')\n",
    "# os.mkdir('./experiments/%s' % run_id)\n",
    "# print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "loader = DataLoader(dataset=TextDataset(dataset), batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nll = 1e30  # set to super large value at first\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
