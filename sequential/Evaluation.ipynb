{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 128 # Number of LSTM layer's neurons\n",
    "D = 10 # Number of input dimension == number of items in vocabulary\n",
    "Z = H + D # Because we will concatenate LSTM state with the input\n",
    "\n",
    "model = dict(\n",
    "    Wf=np.zeros((Z, H)) + 0.1,\n",
    "    Wi=np.zeros((Z, H)) + 0.1,\n",
    "    Wc=np.zeros((Z, H)) + 0.1,\n",
    "    Wo=np.zeros((Z, H)) + 0.1,\n",
    "    Wy=np.zeros((H, D)) + 0.1,\n",
    "\n",
    "    bf=np.zeros((1, H)),\n",
    "    bi=np.zeros((1, H)),\n",
    "    bc=np.zeros((1, H)),\n",
    "    bo=np.zeros((1, H)),\n",
    "    by=np.zeros((1, D))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(inputs):\n",
    "    return inputs * (1 - inputs)\n",
    "\n",
    "def sigmoid(x):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    return sigm\n",
    "\n",
    "def dtanh(inputs):\n",
    "    return 1 - inputs * inputs\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(X, state):\n",
    "    m = model \n",
    "    Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "    bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "    h_old, c_old = state\n",
    "\n",
    "    # One-hot encode\n",
    "    X_one_hot = np.zeros(D)\n",
    "    X_one_hot[X] = 1.\n",
    "    X_one_hot = X_one_hot.reshape(1, -1)\n",
    "\n",
    "    # Concatenate old state with current input\n",
    "    X = np.column_stack((h_old, X_one_hot))\n",
    "    \n",
    "    hf = sigmoid(X @ Wf + bf)\n",
    "    hi = sigmoid(X @ Wi + bi)\n",
    "    ho = sigmoid(X @ Wo + bo)\n",
    "    hc = tanh(X @ Wc + bc)\n",
    "\n",
    "    c = hf * c_old + hi * hc\n",
    "    h = ho * tanh(c)\n",
    "\n",
    "    y = h @ Wy + by\n",
    "    prob = softmax(y)\n",
    "\n",
    "    state = (h, c) # Cache the states of current h & c for next iter\n",
    "    cache = h_old, c_old, hf, hi, ho, hc, h, c, X# Add all intermediate variables to this cache\n",
    "    return prob, state, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X_train, y_train, state):\n",
    "    probs = []\n",
    "    caches = []\n",
    "    loss = 0.\n",
    "    h, c = state\n",
    "\n",
    "    # Forward Step\n",
    "\n",
    "    for x, y_true in zip(X_train, y_train):\n",
    "        prob, state, cache = lstm_forward(x, state)\n",
    "#         loss += cross_entropy(prob, y_true)\n",
    "\n",
    "        # Store forward step result to be used in backward step\n",
    "        probs.append(prob)\n",
    "        caches.append(cache)\n",
    "\n",
    "#     # The loss is the average cross entropy\n",
    "#     loss /= X_train.shape[0]\n",
    "\n",
    "#     # Backward Step\n",
    "\n",
    "#     # Gradient for dh_next and dc_next is zero for the last timestep\n",
    "    d_next = (np.zeros_like(h), np.zeros_like(c))\n",
    "    grads = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "\n",
    "#     # Go backward from the last timestep to the first\n",
    "    for prob, y_true, cache in reversed(list(zip(probs, y_train, caches))):\n",
    "        grad, d_next = lstm_backward(prob, y_true, d_next, cache)\n",
    "\n",
    "\n",
    "#         # Accumulate gradients from all timesteps\n",
    "#         for k in grads.keys():\n",
    "#             grads[k] += grad[k]\n",
    "\n",
    "#     return grads, loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(prob, y_train, d_next, cache):\n",
    "    # Unpack the cache variable to get the intermediate variables used in forward step\n",
    "    dh_next, dc_next = d_next\n",
    "    h_old, c_old, hf, hi, ho, hc, h, c, X = cache\n",
    "    \n",
    "    m = model \n",
    "    Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "    bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "    # Softmax loss gradient\n",
    "    dy = prob.copy()\n",
    "    dy[0, y_train] -= 1.\n",
    "\n",
    "    # Hidden to output gradient\n",
    "    dWy = h.T @ dy\n",
    "    dby = dy\n",
    "    # Note we're adding dh_next here\n",
    "    dh = dy @ Wy.T + dh_next\n",
    "\n",
    "    # Gradient for ho in h = ho * tanh(c)\n",
    "    dho = tanh(c) * dh\n",
    "    dho = dsigmoid(ho) * dho\n",
    "\n",
    "    # Gradient for c in h = ho * tanh(c), note we're adding dc_next here\n",
    "    dc = ho * dh * dtanh(c)\n",
    "    dc = dc + dc_next\n",
    "\n",
    "    # Gradient for hf in c = hf * c_old + hi * hc\n",
    "    dhf = c_old * dc\n",
    "    dhf = dsigmoid(hf) * dhf\n",
    "\n",
    "    # Gradient for hi in c = hf * c_old + hi * hc\n",
    "    dhi = hc * dc\n",
    "    dhi = dsigmoid(hi) * dhi\n",
    "\n",
    "    # Gradient for hc in c = hf * c_old + hi * hc\n",
    "    dhc = hi * dc\n",
    "    dhc = dtanh(hc) * dhc\n",
    "\n",
    "    # Gate gradients, just a normal fully connected layer gradient\n",
    "    dWf = X.T @ dhf\n",
    "    dbf = dhf\n",
    "    dXf = dhf @ Wf.T\n",
    "\n",
    "    dWi = X.T @ dhi\n",
    "    dbi = dhi\n",
    "    dXi = dhi @ Wi.T\n",
    "\n",
    "    dWo = X.T @ dho\n",
    "    dbo = dho\n",
    "    dXo = dho @ Wo.T\n",
    "\n",
    "    dWc = X.T @ dhc\n",
    "    dbc = dhc\n",
    "    dXc = dhc @ Wc.T\n",
    "\n",
    "    # As X was used in multiple gates, the gradient must be accumulated here\n",
    "    dX = dXo + dXc + dXi + dXf\n",
    "    # Split the concatenated X, so that we get our gradient of h_old\n",
    "    dh_next = dX[:, :H]\n",
    "    # Gradient for c_old in c = hf * c_old + hi * hc\n",
    "    dc_next = hf * dc\n",
    "    \n",
    "    grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "    state = (dh_next, dc_next)\n",
    "\n",
    "    return grad, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(0, 10)\n",
    "y_train = np.arange(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.18335579e-19 -1.18335579e-19 -1.18335579e-19 -1.18335579e-19\n",
      " -1.18335579e-19 -1.18335579e-19 -1.18335579e-19 -1.18335579e-19\n",
      " -1.18335579e-19 -1.18335579e-19]\n",
      "[-1.82172611e-19 -1.82172611e-19 -1.82172611e-19 -1.82172611e-19\n",
      " -1.82172611e-19 -1.82172611e-19 -1.82172611e-19 -1.82172611e-19\n",
      " -1.82172611e-19 -1.82172611e-19]\n",
      "[-1.55699414e-19 -1.55699414e-19 -1.55699414e-19 -1.55699414e-19\n",
      " -1.55699414e-19 -1.55699414e-19 -1.55699414e-19 -1.55699414e-19\n",
      " -1.55699414e-19 -1.55699414e-19]\n",
      "[-1.3045654e-19 -1.3045654e-19 -1.3045654e-19 -1.3045654e-19\n",
      " -1.3045654e-19 -1.3045654e-19 -1.3045654e-19 -1.3045654e-19\n",
      " -1.3045654e-19 -1.3045654e-19]\n",
      "[-9.36785907e-20 -9.36785907e-20 -9.36785907e-20 -9.36785907e-20\n",
      " -9.36785907e-20 -9.36785907e-20 -9.36785907e-20 -9.36785907e-20\n",
      " -9.36785907e-20 -9.36785907e-20]\n",
      "[-8.65171077e-20 -8.65171077e-20 -8.65171077e-20 -8.65171077e-20\n",
      " -8.65171077e-20 -8.65171077e-20 -8.65171077e-20 -8.65171077e-20\n",
      " -8.65171077e-20 -8.65171077e-20]\n",
      "[-1.16678312e-18 -1.16678312e-18 -1.16678312e-18 -1.16678312e-18\n",
      " -1.16678312e-18 -1.16678312e-18 -1.16678312e-18 -1.16678312e-18\n",
      " -1.16678312e-18 -1.16678312e-18]\n",
      "[-1.1736151e-15 -1.1736151e-15 -1.1736151e-15 -1.1736151e-15\n",
      " -1.1736151e-15 -1.1736151e-15 -1.1736151e-15 -1.1736151e-15\n",
      " -1.1736151e-15 -1.1736151e-15]\n",
      "[-1.08777982e-14 -1.08777982e-14 -1.08777982e-14 -1.08777982e-14\n",
      " -1.08777982e-14 -1.08777982e-14 -1.08777982e-14 -1.08777982e-14\n",
      " -1.08777982e-14 -1.08777982e-14]\n",
      "[-4.68350822e-14 -4.68350822e-14 -4.68350822e-14 -4.68350822e-14\n",
      " -4.68350822e-14 -4.68350822e-14 -4.68350822e-14 -4.68350822e-14\n",
      " -4.68350822e-14 -4.68350822e-14]\n"
     ]
    }
   ],
   "source": [
    "state = (np.zeros((1, H)), np.zeros((1, H)))\n",
    "train_step(X_train, y_train, state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
